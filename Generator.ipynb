{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as  pd\n",
    "import numpy as np\n",
    "import random\n",
    "import io\n",
    "gpu = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpu[0], True)\n",
    "tf.config.experimental.set_memory_growth(gpu[1], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our deeds are the reason of this hastag earthquake may allah forgive us all \n",
      " forest fire near la ronge sask canada \n",
      " all residents asked to shelter in place are being notified by officers no other evacuation or shelter in place orders are expected \n",
      " 13000 people receive hastag wildfires evacuation orders in california \n",
      " just got sent this photo from ruby hastag alaska as smoke from hastag wildfires pours into a school \n",
      " hastag rockyfire update  california hwy 20 closed in both directions due to\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"df_train_clean.csv\")\n",
    "df_disaster = df.text_clean[df.target == 1].reset_index(drop=True)\n",
    "text = []\n",
    "for row in range(len(df_disaster)):\n",
    "    text.append(df_disaster[row])\n",
    "text = \",\".join(text).replace(\",\",\" \\n \")\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))\n",
    "char_to_ind = {char:i for i, char in enumerate(vocab)}\n",
    "ind_to_char = np.array(vocab)\n",
    "encoded_text = np.array([char_to_ind[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o\n",
      "u\n",
      "r\n",
      " \n",
      "d\n",
      "e\n",
      "e\n",
      "d\n",
      "s\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Create Training Sequences\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n",
    "\n",
    "for i in char_dataset.take(10):\n",
    "     print(ind_to_char[i.numpy()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = len(vocab)\n",
    "sequences = char_dataset.batch(seq_len+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function create_seq_targets at 0x000002610000C280> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function create_seq_targets at 0x000002610000C280> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "[26 32 29  1 15 16 16 15 30  1 12 29 16  1 31 19 16  1 29 16 12 30 26 25\n",
      "  1 26 17  1 31 19 20 30  1 19 12 30 31 12 18  1 16 12 29 31 19 28 32 12\n",
      " 22 16  1]\n",
      "our deeds are the reason of this hastag earthquake \n",
      "\n",
      "\n",
      "[32 29  1 15 16 16 15 30  1 12 29 16  1 31 19 16  1 29 16 12 30 26 25  1\n",
      " 26 17  1 31 19 20 30  1 19 12 30 31 12 18  1 16 12 29 31 19 28 32 12 22\n",
      " 16  1 24]\n",
      "ur deeds are the reason of this hastag earthquake m\n"
     ]
    }
   ],
   "source": [
    "def create_seq_targets(seq):\n",
    "    input_txt = seq[:-1]\n",
    "    target_txt = seq[1:]\n",
    "    return input_txt, target_txt\n",
    "    \n",
    "dataset = sequences.map(create_seq_targets)\n",
    "\n",
    "for input_txt, target_txt in  dataset.take(1):\n",
    "    print(input_txt.numpy())\n",
    "    print(''.join(ind_to_char[input_txt.numpy()]))\n",
    "    print('\\n')\n",
    "    print(target_txt.numpy())\n",
    "    # There is an extra whitespace!\n",
    "    print(''.join(ind_to_char[target_txt.numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "batch_size = 50\n",
    "\n",
    "# Buffer size to shuffle the dataset so it doesn't attempt to shuffle\n",
    "# the entire sequence in memory. Instead, it maintains a buffer in which it shuffles elements\n",
    "buffer_size = 10000\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "101/101 [==============================] - 20s 165ms/step - loss: 3.7836\n",
      "Epoch 2/60\n",
      "101/101 [==============================] - 17s 165ms/step - loss: 2.7340\n",
      "Epoch 3/60\n",
      "101/101 [==============================] - 17s 164ms/step - loss: 2.5016\n",
      "Epoch 4/60\n",
      "101/101 [==============================] - 17s 164ms/step - loss: 2.3525\n",
      "Epoch 5/60\n",
      "101/101 [==============================] - 17s 165ms/step - loss: 2.2290\n",
      "Epoch 6/60\n",
      "101/101 [==============================] - 17s 164ms/step - loss: 2.1115\n",
      "Epoch 7/60\n",
      "101/101 [==============================] - 17s 164ms/step - loss: 1.9796\n",
      "Epoch 8/60\n",
      "101/101 [==============================] - 17s 165ms/step - loss: 1.8482\n",
      "Epoch 9/60\n",
      "101/101 [==============================] - 17s 167ms/step - loss: 1.7279\n",
      "Epoch 10/60\n",
      "101/101 [==============================] - 17s 165ms/step - loss: 1.6222\n",
      "Epoch 11/60\n",
      "101/101 [==============================] - 17s 165ms/step - loss: 1.5168\n",
      "Epoch 12/60\n",
      "101/101 [==============================] - 17s 167ms/step - loss: 1.3932\n",
      "Epoch 13/60\n",
      "101/101 [==============================] - 17s 165ms/step - loss: 1.2668\n",
      "Epoch 14/60\n",
      "101/101 [==============================] - 17s 165ms/step - loss: 1.1372\n",
      "Epoch 15/60\n",
      "101/101 [==============================] - 17s 165ms/step - loss: 0.9872\n",
      "Epoch 16/60\n",
      "101/101 [==============================] - 17s 166ms/step - loss: 0.8505\n",
      "Epoch 17/60\n",
      "101/101 [==============================] - 17s 165ms/step - loss: 0.7297\n",
      "Epoch 18/60\n",
      "101/101 [==============================] - 17s 165ms/step - loss: 0.6389\n",
      "Epoch 19/60\n",
      "101/101 [==============================] - 17s 167ms/step - loss: 0.5707\n",
      "Epoch 20/60\n",
      "101/101 [==============================] - 16s 157ms/step - loss: 0.5189\n",
      "Epoch 21/60\n",
      "101/101 [==============================] - 16s 154ms/step - loss: 0.4858\n",
      "Epoch 22/60\n",
      "101/101 [==============================] - 16s 156ms/step - loss: 0.4564\n",
      "Epoch 23/60\n",
      "101/101 [==============================] - 17s 162ms/step - loss: 0.4313\n",
      "Epoch 24/60\n",
      "101/101 [==============================] - 16s 155ms/step - loss: 0.4137\n",
      "Epoch 25/60\n",
      "101/101 [==============================] - 16s 155ms/step - loss: 0.3965\n",
      "Epoch 26/60\n",
      "101/101 [==============================] - 16s 155ms/step - loss: 0.3820\n",
      "Epoch 27/60\n",
      "101/101 [==============================] - 16s 155ms/step - loss: 0.3710\n",
      "Epoch 28/60\n",
      "101/101 [==============================] - 16s 154ms/step - loss: 0.3572\n",
      "Epoch 29/60\n",
      "101/101 [==============================] - 16s 154ms/step - loss: 0.3496\n",
      "Epoch 30/60\n",
      "101/101 [==============================] - 16s 156ms/step - loss: 0.3429\n",
      "Epoch 31/60\n",
      "101/101 [==============================] - 16s 159ms/step - loss: 0.3322\n",
      "Epoch 32/60\n",
      "101/101 [==============================] - 16s 155ms/step - loss: 0.3271\n",
      "Epoch 33/60\n",
      "101/101 [==============================] - 16s 155ms/step - loss: 0.3202\n",
      "Epoch 34/60\n",
      "101/101 [==============================] - 17s 164ms/step - loss: 0.3174\n",
      "Epoch 35/60\n",
      "101/101 [==============================] - 16s 158ms/step - loss: 0.31010s - loss:\n",
      "Epoch 36/60\n",
      "101/101 [==============================] - 17s 161ms/step - loss: 0.3049\n",
      "Epoch 37/60\n",
      "101/101 [==============================] - 17s 163ms/step - loss: 0.3019\n",
      "Epoch 38/60\n",
      "101/101 [==============================] - 17s 162ms/step - loss: 0.3001\n",
      "Epoch 39/60\n",
      "101/101 [==============================] - 17s 162ms/step - loss: 0.2921\n",
      "Epoch 40/60\n",
      "101/101 [==============================] - 17s 161ms/step - loss: 0.2888\n",
      "Epoch 41/60\n",
      "101/101 [==============================] - 16s 159ms/step - loss: 0.2875\n",
      "Epoch 42/60\n",
      "101/101 [==============================] - 16s 154ms/step - loss: 0.2834\n",
      "Epoch 43/60\n",
      "101/101 [==============================] - 16s 153ms/step - loss: 0.2820\n",
      "Epoch 44/60\n",
      "101/101 [==============================] - 16s 153ms/step - loss: 0.2794\n",
      "Epoch 45/60\n",
      "101/101 [==============================] - 16s 157ms/step - loss: 0.2747\n",
      "Epoch 46/60\n",
      "101/101 [==============================] - 16s 153ms/step - loss: 0.2746\n",
      "Epoch 47/60\n",
      "101/101 [==============================] - 16s 154ms/step - loss: 0.2703\n",
      "Epoch 48/60\n",
      "101/101 [==============================] - 16s 154ms/step - loss: 0.2707\n",
      "Epoch 49/60\n",
      "101/101 [==============================] - 16s 157ms/step - loss: 0.2687\n",
      "Epoch 50/60\n",
      "101/101 [==============================] - 16s 154ms/step - loss: 0.2657\n",
      "Epoch 51/60\n",
      "101/101 [==============================] - 16s 154ms/step - loss: 0.2667\n",
      "Epoch 52/60\n",
      "101/101 [==============================] - 16s 154ms/step - loss: 0.2647\n",
      "Epoch 53/60\n",
      "101/101 [==============================] - 16s 157ms/step - loss: 0.2639\n",
      "Epoch 54/60\n",
      "101/101 [==============================] - 16s 153ms/step - loss: 0.2612\n",
      "Epoch 55/60\n",
      "101/101 [==============================] - 16s 151ms/step - loss: 0.2654\n",
      "Epoch 56/60\n",
      "101/101 [==============================] - 16s 152ms/step - loss: 0.2630\n",
      "Epoch 57/60\n",
      "101/101 [==============================] - 16s 153ms/step - loss: 0.2603\n",
      "Epoch 58/60\n",
      "101/101 [==============================] - 16s 152ms/step - loss: 0.2618\n",
      "Epoch 59/60\n",
      "101/101 [==============================] - 16s 152ms/step - loss: 0.2595\n",
      "Epoch 60/60\n",
      "101/101 [==============================] - 16s 156ms/step - loss: 0.26302s - loss: 0.26 - ETA: 2s - loss: 0.261 - ETA: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x261229c0190>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def create_model(vocab_size, embed_dim, rnn_neurons, batch_size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(vocab_size, embed_dim,batch_input_shape=[batch_size, None]))\n",
    "    model.add(tf.keras.layers.LSTM(rnn_neurons,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "    # Final Dense Layer to Predict\n",
    "    model.add(tf.keras.layers.Dense(vocab_size))\n",
    "    model.compile(optimizer='adam', loss=loss) \n",
    "    return model\n",
    "\n",
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "# The embedding dimension\n",
    "embed_dim = 64\n",
    "# Number of RNN units\n",
    "rnn_neurons =4154\n",
    "\n",
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "#Create the model\n",
    "model = create_model(\n",
    "  vocab_size = vocab_size,\n",
    "  embed_dim=embed_dim,\n",
    "  rnn_neurons=rnn_neurons,\n",
    "  batch_size=batch_size)\n",
    "\n",
    "\n",
    "#Train the model\n",
    "epochs = 60\n",
    "model.fit(dataset,epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# news # chech # bitcoing # blockchain the midsouth and midwest   # weather \n",
      " rock springs ûò residents land cry spreading fire # nightbeat veronicadly outbreak \n",
      " 10th death confirmed in legionnaires outbreak in southbbombed port north robat \n",
      " bin laden family plane crashed after avoiding microlight and landing too far down runway \n",
      " this guy bought mestrump is a climate denier algae bloom in the pacific liber now \n",
      " bluebirddenver # fettilootch i # yyc # yyc # abstorm \n",
      " ready for backern california   g \n",
      " there was a small earthquake in la but dont worry emmy rossum is fine \n",
      " sure the cameroon repatriating about 12000 police post in udhampur 2 police look back police post in udhampur 2 spos injured  livemintag zo # virwill and mainline train derails \n",
      " hotel evacuated after landslide in the italian alps   via todayngr \n",
      " faan giver in a layer of grit \n",
      " sandstorm woo hoo \n",
      " socialwots militants attack police post 2 spos injuredû   # just is too short \n",
      " in this fragile global economy consitned next week    event w damage from a tornado oles in most   spriandatch every story pould storm ofd \n",
      " vinustrip did you see # hurricane # guillermo \n",
      " pt the pain of those seconds must have been awful as hurricane mass murder that to evacuate a few times and it \n",
      " burning buildings media outrage \n",
      " must got her lost 90 of emergency evacuation happening now in the building # mht \n",
      " hinton city officials confirm multiple pkk suicide bomber who detonated bomb in turkey army trench released \n",
      " 11 soldiers killed in which # roomsupportellowers \n",
      " so you have to do is lofived hima and # nagasaki # otd \n",
      " we tuney ough texas \n",
      " dust in thquake \n",
      " # sismo detectado # japìn report 1 010156 okinawa island region m38 depth 10km maximum seismic intensity 3 jst # hiroshima \n",
      " news786uk islamist clear thero ed by structural failure after the copilot \n",
      " investigators shift focus to cause of fatal waiman has coated the middle eastûªs largest refugee camperslings \n",
      " images of famine ûò hope in christ  to the whole sky being red after a dust stormlike a via youtube \n",
      " landslide kills three near venice after les   via trafford \n",
      " japan aogashima volcano by unknown  check it out \n",
      " fel lords from another theyre \n",
      " # worldnews fallen powers in ore app for more information \n",
      " eruption of indonesianal forest arad and bridge which likely to collapse \n",
      " ashes 2015 australia collision on broadway at least two people wt drought conditions in your area \n",
      " the my hostages not particularly pleasantly if div 30 fougif the whole country s endone rescuers sourd for conferns racks of # nuclear disaster in western # japan # nuclear safety standard is farmliteration # demonization \n",
      " thatsabinegirl you derailed at smithsonian \n",
      " metro acting chief jacking # nowplayed following false fire alarm  twi   # manchester \n",
      " full reû \n",
      " a small flood with bigåêconsequences us forest service says spending more than half of business m being mist injured in deadly saudi mosque suicide attack \n",
      " see side bombing in suruì that killed 32 people turkey last food of f hail after 2  hit the most damage \n",
      " the g \n",
      " gov murderer and white guy tree her ing dispuned after state of denial naved the teather one i did another one you still ain plug us and detonate their bombs \n",
      " apollo brown  detonate ft mop  lta # business \n",
      " # bedition plan incpoduction magics fine just dont gone to warped today rosk tragged under an 12000 nigerian refugees repatriated from cameroon \n",
      " reaad plss 300 people capsized off the coast ofû \n",
      " # news bin laden family plane crashed after avoiding microlight and landing too far down runway \n",
      " this guy bought m into distance \n",
      " crazy mom threw teen daughter a nude   # aogashima # japan # photography # arts \n",
      " be ember aware \n",
      " columbia right man at the helm can saveshould just be them strutting like sees the hospate from the financial bayelsa as patience jonathan plans to hijack apcåêpdp \n",
      " swellyje areavoluntaryinciwebmad river complex announcement of most aviation cr \n",
      " experts in france begin examining airplane debris found on reunion island french air accident experts on wedn \n",
      " alexalltimelow american casualties by iranian activity suspected militants attack police post 2 spos injuredû   # rockent \n",
      " california brews rubbom is a queen in # apolamic bomb exploded \n",
      " # illinois emergency evatu# russian food crematoria provokes outrage amid crisis famineåêresult \n",
      " on eaus saudi arabian mosque  the i \n",
      " suicide bomber kills 15 in \n",
      " ar  severe thunderstorm warning issued august 05 at 911pm cdt until august 05 at 911pm cdt until august and people are reported to be for a breaking news confirmed as from missing flight mh370 via yahoonearold boy cia is some oomous # 17 \n",
      " junko was 13 years old when the atomic bombs # hiroshima 3 days later it was almost loud ban with low violation carreith bombed the asong r\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import random\n",
    "model.save('tweets_gen.h5') \n",
    "\n",
    "#Currently our model only expects 128 sequences at a time. We can create a new model that only expects a batch_size=1. We can create a new model with this batch size, then load our saved models weights.\n",
    "#Then call .build() on the mode\n",
    "\n",
    "model = create_model(vocab_size, embed_dim, rnn_neurons, batch_size=1)\n",
    "model.load_weights('tweets_gen.h5')\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "\n",
    "def generate_text(model, start_seed,gen_size=100,temp=0.95):\n",
    "    '''\n",
    "    model: Trained Model to Generate Text\n",
    "    start_seed: Intial Seed text in string form\n",
    "    gen_size: Number of characters to generate\n",
    "    Basic idea behind this function is to take in some seed text, format it so\n",
    "    that it is in the correct shape for our network, then loop the sequence as\n",
    "    we keep adding our own predicted characters. Similar to our work in the RNN\n",
    "    time series problems.\n",
    "    '''\n",
    "    # Number of characters to generate\n",
    "    num_generate = gen_size\n",
    "    # Vecotrizing starting seed text\n",
    "    input_eval = [char_to_ind[s] for s in start_seed]\n",
    "    # Expand to match batch format shape\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    # Empty list to hold resulting generated text\n",
    "    text_generated = []\n",
    "    # Temperature effects randomness in our resulting text\n",
    "    # The term is derived from entropy/thermodynamics.\n",
    "    # The temperature is used to effect probability of next characters.\n",
    "    # Higher probability == lesss surprising/ more expected\n",
    "    # Lower temperature == more surprising / less expected\n",
    "    temperature = temp\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "      # Generate Predictions\n",
    "      predictions = model(input_eval)\n",
    "      # Remove the batch shape dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "      # Use a cateogircal disitribution to select the next character\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "      # Pass the predicted charracter for the next input\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "      # Transform back to character letter\n",
    "      text_generated.append(ind_to_char[predicted_id])\n",
    "    return (start_seed + ''.join(text_generated))\n",
    "\n",
    "\n",
    "print(generate_text(model,random.choice(text.split()),gen_size=5000).replace('hastag','#'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
